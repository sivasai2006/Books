import org.apache.spark.sql.SparkSession

val sparkSession = SparkSession.builder().appName("SparkSessionZipsExample").config(conf).getOrCreate()

//https://github.com/phatak-dev/spark2.0-examples/blob/master/src/main/resources/sales.csv
val df1 = sparkSession.read.option("header", "true").csv("src/main/resources/sales.csv")
val df2 = sparkSession.read.option("header", "true").csv("src/main/resources/sales.csv")

val joinExpression = df1.col("customerId") === df2.col("customerId")
var joinType = "inner"

/*Inner joins are the default join, so we just need to specify our left DataFrame and join the right in the JOIN expression: 
df1.join(df2, joinExpression).show()*/
var df3 = df1.join(df2, joinExpression, joinType).show()

//Anomynous function which takes an Integer and returns the Category
val addColumn : (Int)=>Int=(numValue:Int)=>(numValue: @switch) match {
        case 1  => "One"
        case 2  => "Two"
        case _  => "Other"
    }
	
df3.withColumn("customerCategory", expr("customerId > 10")).show(2)